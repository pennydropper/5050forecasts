---
title: Measuring Demand Planning Behaviours
author: James Northrop
date: '2019-09-15'
slug: measuring-demand-planning-behaviours
categories:
  - Sales and Operations Planning
  - Process Maturity
  - demand planning and forecasting
tags:
  - Agile
  - Continuous Improvement
  - FMCG
  - forecast
  - Sales and operation planning
  - supply planning
  - uncertainty
  
draft: false

thumbnailImagePosition: left
thumbnailImage: /img/Business_process_s_97045361.jpg # //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city-750.jpg
coverImage: /img/Business_process_s_97045361.jpg # //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city.jpg
coverSize: partial
metaAlignment: center
coverMeta: out

comments: true
favicon: "/img/favicon-32x32.png"
---



<p>What’s better; making the wrong decision for the right reasons or making the right decision for the wrong reasons? If it’s one-off, then the right decision for the wrong reasons (also known as a fluke), is likely to receive plaudits. However, if it’s part of a decision making process recurring every month, such as sales forecasting, then I’ll back the wrong decision for the right reasons (also known as bad luck) every time. We can expect that the cumulative outcomes of the right process will significantly exceed those from the wrong process.</p>
<p>The key point of this blog is that business should measure forecasting performance as much on measuring the process against the ideal as it should on measuring sales versus forecast.</p>
<p><strong>Right Decision for the Wrong Reasons?</strong><br />
One of my more challenging episodes as a demand planner was to manage expectations around an end of financial year sales drive in two consecutive years. This sales drive idea, conceived by the Sales Director himself, was based around promoting local wines in each state. The volume uplift was driven mainly by distribution growth, product placement, point of sale mechanics and some multi-buy discounting. With heavy investment, the forecast uplift was substantial but unsupported by the outcomes of previous activities. On top of this, the Sales Director scrutinised every sales forecast from every state and reacted aggressively to every forecast that fell short of the targets. Me, the messenger, was told that I was being soft on the states and that I was providing them with an excuse to under-deliver.</p>
<p>In both years, the sales volumes fell short of the total targets though some individual products did achieve their targets. The successful results of the few products that did achieve their target was, implicitly, used to justify the program. In other words, those few successes were used as examples of having made the right decision. On the other hand, the outcome that the majority of products that failed to achieve their forecasts is evidence not only that the process was wrong for those products, but that even the process for the few succeeding products was the wrong process.</p>
<p>The outcome of the sales promotions, in both years, was a slight uplift in sales revenue from the standard run-rate but a significant increase in stock-on-hand as total sales fell short of the “forecast”.</p>
<p><strong>The Right Process, aka the right reasons</strong><br />
So, what are the crucial behaviours and processes that lead to a forecast made for the right reasons? What are the characteristics of a mature and high functional forecasting process that lead to forecasting decisions made for the right reasons. Here’s a list I’ve compiled that applies to FMCG products.</p>
<p>Activity planning that respects supply lead times and supply capacity.</p>
<ul>
<li>Supplier and retailer led.</li>
<li>Product allocations</li>
</ul>
<p>Shipment forecasting that factors customer stock-on-hand.</p>
<p>Diligent assembly of assumptions, from the market level down.</p>
<ul>
<li>What are the market level trends?</li>
<li>Does the activity plan align to retailer capacity?</li>
</ul>
<p>Bottom up forecasts that align with top-down forecasts</p>
<ul>
<li>Forecasted market share is consistent with trends.</li>
<li>Cannibalisation (and possible halo-uplift) from related activity factored into SKU forecasts.</li>
</ul>
<p>Root cause analysis of more significant forecast errors</p>
<ul>
<li>Learnings are captured and put into action.</li>
<li>Outcomes against assumptions are documentented and assumption methodology is evolves appropriately.</li>
</ul>
<p>Forecasts supported by data and statistical models</p>
<ul>
<li>Where appropriate.</li>
<li>e.g. promotional uplift reflects numerous prior promotions where possible, not just the most recent.</li>
</ul>
<p>Forecast uncertainty communicated to stakeholders</p>
<p><strong>Numerous Benefits Compared to Simple Forecast Accuracy</strong><br />
The beauty of this new approach to measuring forecasting performance based on behaviours is that it is adaptable to the local domain. Thus, it is customisable to suit the particular needs of forecasting stakeholders.</p>
<blockquote>
<p>The beauty of this new approach to measuring forecasting performance based on behaviours is that it is adaptable to the local domain.</p>
</blockquote>
<p>Another benefit compared to the traditional sales versus forecast method is that it does not prioritise one lag period (usually 1 month) to the exclusion of all others. In fact, most forecasting performance measures only factor a one-month lag, implicitly allowing users to pay less attention to other forecast horizons.</p>
<p>Another benefit of this method is the direct relationship between the forecasting performance score and the improvement opportunity. For example, feedback that the forecast uncertainty was not communicated in the agreed format is self explanatory, unlike, for instance a MAPE score of 30% compared to a target of 28% which hardly communicates any insights on the impact of forecast users.</p>
<p><strong>How to Implement</strong><br />
My suggestion for how to implement this new measure is to score the forecasting behaviours at the end of every forecasting cycle for that forecast period based on the latest information. The key question under each heading is “was sufficient due diligence applied to this aspect of forecasting?”. My suggestion is that the default assumption is that due diligence was applied except by exception. That is, assume a perfect score under each heading except when an oversight is detected or raised.</p>
<blockquote>
<p>The key question under each heading is “was sufficient due diligence applied to this aspect of forecasting?”</p>
</blockquote>
<p>A challenge to this method is that it requires a blend of judgement and objectivity. In other words, one player’s expectations of due diligence may not align with another player’s expectations. The key to mitigating this vulnerability is to document the judgements made and to consciously and conspicuously pursue consistency. Another opportunity to achieving consistency and some objectivity is to appoint someone to take the role of critic to balance any self-congratulatory bias.</p>
<ol style="list-style-type: decimal">
<li><p>Adopt an <a href="../../../2019/04/the-agile-opportunity-for-demand-planning-improvements/" target="_blank">Agile approach</a> to rolling this out iteratively.</p></li>
<li><p>Develop a basic but fundamental set of expected mature process traits. More nuanced maturity traits should come in later iterations.</p></li>
<li><p>Develop a register for recording performance against maturity traits.</p></li>
<li><p>Establish a recurring responsibility and process for reviewing behaviours against each of the maturity traits.</p></li>
<li><p>Establish a recurring responsibility and process for reviewing and updating the maturity definition, based on emerging issues and opportunities.</p></li>
</ol>
<p><strong>In Summary</strong><br />
Measuring forecasting performance purely on forecast accuracy is misleading as it overlooks the random (luck) element of sales outcomes. In addition, the forecast accuracy measure is just a number and conveys no message about the underlying issues nor the effect of poor forecasting on stakeholders.</p>
<p>An alternative presented in this blog outlines a method of measuring forecasting performance based on behaviours that drive outcomes, akin to measuring behaviours against a process maturity model.</p>
<p>A strong benefit with basing forecasting performance on behaviours is that it focuses attention on the desired behaviours and is customisable to environment in which the business operates.</p>
