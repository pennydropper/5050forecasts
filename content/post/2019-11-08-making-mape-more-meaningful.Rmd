---
title: Making MAPE More Meaningful
author: James Northrop
date: '2019-11-08'
slug: making-mape-more-meaningful
categories:
  - Process Maturity
  - demand planning and forecasting
tags:
  - Agile
  - Continuous Improvement
  - Finance
  - forecast
  - Sales and operation planning
  - Stakeholder management
  - Supply
  - uncertainty

draft: false
  
thumbnailImagePosition: left
thumbnailImage: /img/businessman-blindfold-hitting-target-bow-arrow-illustration-47052508.jpg # //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city-750.jpg
coverImage: /img/businessman-blindfold-hitting-target-bow-arrow-illustration-47052508.jpg # //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city.jpg
coverSize: partial
metaAlignment: center
coverMeta: out

comments: true
favicon: "/img/favicon-32x32.png"
---
“You can’t manage what you can’t measure”, which is why it’s important for businesses to measure forecast accuracy.  For those unaware, MAPE (Mean Absolute Percentage Error) is one of the three most common forecast accuracy measures along with bias and MSE.  It is also a proxy for measuring the demand planning value-add in an FMCG businesses. That is, when the MAPE score is favourable (low), then demand planning’s value-add for a business must be high.

```{r setup, echo=FALSE, eval=FALSE, message=FALSE}
# require(tidyverse)
# require(ggthemes)
# require(lubridate)

knitr::opts_chunk$set(package.startup.message = FALSE)
library <- function(...) suppressPackageStartupMessages(base::library(...))

# suppressPackageStartupMessages(source("../../code/functions.R"))

library(tidyverse)
library(ggthemes)
library(lubridate)
library(magrittr)

```


```{r Ideal MAPE vs value-add, echo=FALSE, warning=FALSE, message=FALSE}

library(tidyverse)
library(ggthemes)
library(lubridate)
library(magrittr)

plot_mape_val_add <- function(p_title = "",  a = -2 * 1e3, b = 2 * 1e5, rand_f = 0.1) {
  # Builds a plot of relationship between MAPE scores and business value-add
  set.seed(1234)
  
  mape_val_add <-
    tibble(mape = runif(24, 25, 36)) %>% 
    mutate(val_add = b + a * mape - rand_f * a * rnorm(length(mape)))
  
  mape_val_rsq <- cor(mape_val_add$mape, mape_val_add$val_add) ^ 2
  
  mape_val_add %>% 
    ggplot(aes(x = mape / 100, y = val_add)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    theme_economist() +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = NULL) +
    labs(title = p_title,
         subtitle = str_c("R-squared: ", format(mape_val_rsq, digits = 2)),
         x = "MAPE score", y = "Business Value")
}

plot_mape_val_add("Presumed MAPE score and Business Value Relationship",
                  -2000, 20000, 1)


```


MAPE measures the forecast accuracy at a product or SKU level; it highlights forecasting issues where forecasts for individual products vary considerably from actual quantities, even if the bias for the total forecast is close to the ideal zero score.  For example, a MAPE score of 30% indicates that the forecast sales for individual products was either 30% above or below the actual sales (or visa versa if you divide actual by forecast).

Apart from bias, the other frequently used family of forecast scores is MSE, Mean Squared Error, which is more statistically pure though less meaningful to a wider audience than MAPE.  Compared to MAPE, MSE significantly penalises larger forecast errors.

I recommend my blog on [Forecasts are Deliberately Wrong](https://5050forecasting.netlify.com/2019/02/forecasts-are-deliberately-wrong/){target="_blank"} for my take on the vital importance of the bias measure of forecast accuracy.

MAPE (and MSE) are simple measures for forecast accuracy and definitely play a useful role. In fact, the simplicity is its key strength and why it is used so widely.  However, MAPE is a blunt instrument in a demand planning environment and holds 4 major weaknesses in how it is often applied.  In fact, these weaknesses mean that MAPE is a surprisingly poor measure of demand planning’s value-add.  In this piece, I’ll go onto describe each of these weaknesses and progress to suggesting an alternative.

```{r Alternative MAPE vs value-add, echo=FALSE, warning=FALSE}

plot_mape_val_add("More likely MAPE score and Business Value Relationship",
                  -2000, 20000, 7.5)

```


By the way, I’m classing forecast accuracy measures like WMAPE, SMAPE and r-squared as close cousins of either MAPE or MSE and subject to the same 4 weaknesses listed below.

**Strength 1: MAPE is simple and easy to calculate**  
By far the biggest strength of the MAPE statistic is that it is easy to calculate so long as you have stored the forecast at a SKU level and have accurate actual sales figures. Excel is capable of calculating the MAPE statistic in less than a minute, even if a pivot table is required to sum some components.
 
**Weakness 1: Most MAPE measures only focus on 1 lag period**  
Most MAPE measures I’ve seen were based either on a 1-month or a 3-month lag, i.e. comparing the forecast 1 month or 3 months in advance of the forecast period.  The problem is that it disregards the importance of the forecasts with other lag periods.  If the other lag periods are disregarded, how can an organisation know how well the forecasting role is functioning?

```{r fcst sparklines, echo=FALSE, warning=FALSE}
month_start <- "1/1/2018"
ampl <- 100
rand_c <- 33
level <- 1000

set.seed(1234)

fcst_snaps <-
  crossing(tibble(fcst_mnth = 0:12), # only 12 months required
           tibble(mnth = 0:36)) %>%
  filter(mnth >= fcst_mnth, # Lag >= 0
         mnth - fcst_mnth <= 24) %>%   # 24 month horizon
  mutate(fcst = level + ampl * sin(mnth / 12 * 2 * pi) + rand_c * rnorm(length(mnth)),
         fcst_lag = mnth - fcst_mnth,
         fcst_mnth = dmy(month_start) + months(fcst_mnth),
         mnth = dmy(month_start) + months(mnth))

fcst_sparklines <- function(p_data) {
  # Draw sparklines for the forecast data
  
  p_data <-
    p_data %>%  
    mutate(fcst = round(fcst,0),
           fcst_min = min(fcst),
           fcst_max = max(fcst),
           facet_l = format(fcst_mnth, "%b-%y"))
  
  mnth_lbl <-
    p_data %>% 
    select(fcst_mnth, facet_l) %>% 
    distinct() %>% 
    deframe()
  
  mins <- group_by(p_data, fcst_mnth) %>% slice(which.min(fcst))
  maxs <- group_by(p_data, fcst_mnth) %>% slice(which.max(fcst))
  ends <- group_by(p_data, fcst_mnth) %>% filter(mnth == max(mnth))
  jan <- p_data %>% filter(mnth == ymd("2019/1/1"))
  # quarts <- p_data %>% group_by(fcst_mnth) %>%
  #   summarize(quart1 = quantile(fcst, 0.25),
  #             quart2 = quantile(fcst, 0.75)) %>%
  #   right_join(p_data)
  fcst_min <- min(p_data$fcst) %>% as.integer()
  
  p_data %>% 
    ggplot(., aes(x=mnth, y=fcst)) + 
    facet_grid(fcst_mnth ~ ., switch = "y", labeller = labeller(fcst_mnth = mnth_lbl)) + 
    geom_rect(aes(xmin = ymd("2019/1/1") - 15, xmax = ymd("2019/1/1") + 15, ymin = fcst_min, ymax = fcst_max + 20L), fill = 'grey90') +
    geom_line(size=0.3) +
    geom_point(data = mins, col = 'red', size = .5) +
    geom_point(data = maxs, col = 'blue', size = .5) +
    geom_point(data = jan, size = .8) +
    expand_limits(x = max(p_data$mnth) + (0.25 * (max(p_data$mnth) - min(p_data$mnth)))) +
    scale_x_date(breaks = dmy(month_start) + months(seq(0, 36, 12)), date_labels = "%b-%y") +
    scale_y_continuous(expand = c(0.1, 0)) +
    theme_tufte(base_size = 15, base_family = "Helvetica") +
    theme(axis.title=element_blank(), axis.text.y = element_blank(), axis.text.x = element_text(size = 8),
          axis.ticks = element_blank(), strip.text.y = element_text(angle = 180, size = 8),
          plot.title = element_text(size = 10)) +
    
    geom_text(data = jan, aes(label = fcst), nudge_x = 50, size = 3) +
    labs(title = "Evolving Jan-19 forecast with each S&OP cycle")
  
}

fcst_snaps %>% 
  fcst_sparklines()


```

```{r jan19 fcst bar, echo=FALSE, warning=FALSE}
# require(plotly)

fcst_snap_ggp <-
  fcst_snaps %>% 
  filter(mnth == dmy("1/1/2019")) %>% 
  mutate(mape_mnth = if_else(fcst_lag == 1, "MAPE", "not MAPE")) %>% 
  ggplot(aes(x = fcst_mnth, y = fcst)) +
  geom_bar(aes(fill = mape_mnth), stat = "identity") +
  labs(title = "Which S&OP cycles contribute to Jan-19's MAPE?",
       x = "Forecast month", y = "forecast quantity") +
  scale_fill_discrete("") +
  geom_text(data = fcst_snaps %>% filter(fcst_lag == 1, mnth == dmy("1/1/2019")), aes(label = fcst %>% scales::comma()), 
            nudge_y = 10) +
  coord_cartesian(ylim = c(800, 1100)) +
  theme_economist()

# plotly::ggplotly(fcst_snap_ggp)
fcst_snap_ggp

```
Even if a month’s forecast is accurate, the MAPE score disregards potentially critical weekly splits.  For instance, there is a significant difference if the entire month’s requirement is taken in the first week of the month as opposed to the last week of the month.  One example from my career occurred when a supermarket chain planned a promotion for a product imported from New Zealand, i.e. with extended lead times.  In this example, the promotion (just) met the required 1-month lead time but it resulted in an out-of-stock as it was required entirely in the first week of the month.

**Weakness 2: The delay between the forecasting activity and measuring the outcome of the activity**  
Since MAPE requires a lag, often 1 or 3 months, there is a delay between the activity that leads to a forecast and measuring the accuracy of that forecast, i.e. 1 or 3 months depending on the lag.  If MAPE is the primary measure for measuring the performance of the demand planning team, this delay or feedback lag hinders process improvement.  This delay is particularly difficult if MAPE is included in staff performance assessments and key performance outcomes.

![](/img/Delay to measuring MAPE.png)

**Weakness 3: A MAPE score is relatively meaningless to the wider audience**  
Credit goes to a former Sales Director (DC) for raising this issue.  When the monthly Demand Review presented the MAPE score achieved for the previous month, he asserted that it didn’t mean anything to him.  Quite reasonably, he asked what impact the MAPE score would have on the business bottom line.  The “bottom line” question is almost impossible to answer based purely on the MAPE score.

**Weakness 4: An individual MAPE score is an unreliable indicator of forecasting methods**  
Consider a situation where I forecast 50 cars to go past a marker in 30 minutes on a country road and 50 cars actually go past.  Does that mean that I have forecasted well?  Let’s say that 65 cars go past in the next 30 minutes compared to my forecast of 50, i.e. an error of 30%; has my forecasting method failed or am I just unlucky?

A robust method for forecasting the number of cars on a country road may consider the following:  

* Historical data on the number of cars that pass.
* Time of day.
* Extenuating circumstances, such as school holidays or whether there is a local football match down the road.
* The weather forecast.

FMCG environments will see fluctuating MAPE result; for example, they may fluctuate between 25% and 35%.  Does a one-off MAPE result of, say, 35%, indicate that forecasting performed particularly poorly in that month (the wrong forecast for the wrong reasons) or was it the result of dumb, bad luck (the wrong forecast for the right reasons)?  My point is that a forecast accuracy measure that focuses only on the quantities and not methods or behaviours then fails to factor the random aspect of an outcome.

```{r distn of MAPE scores, echo=FALSE, warning=FALSE}
set.seed(1234)

tibble(mape_int = seq(0.24, 0.36, 0.01)) %>% 
       mutate(mape_rescale = scales::rescale(mape_int),
              prob_int = dweibull(mape_rescale * 1.5, 4, 1),
              prob_int = prob_int / sum(prob_int)) %>% 
  # print
  ggplot(aes(x = mape_int , y = prob_int)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(labels = scales::percent, breaks = seq(0.24, 0.36, 0.02)) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.2)) +
  labs(title = "Random effect of MAPE score", subtitle = "Range of possible results even with same forecasting method",
       x = "MAPE score", y = "Probability of MAPE outcome") +
  theme_economist()

```


**Weakness 4a: It takes a number of cycles of MAPE to confidently judge forecasting methods**  
Even with robust forecasting, some forecast error is likely.  I would take a number of forecast cycles to appreciate if the method was effective or whether the method included hidden flaws.

In a commercial terms, one month’s MAPE score is not a good indicator of the integrity of the forecasting method - it may just be that the system was particularly lucky or unlucky.  Even 2 consecutive months of a particularly good or bad MAPE score shouldn’t lead to a confident inference that the forecasting method is performing well.

**I recommend reporting 3 forecasting performance measures**  
I recommend that demand planning reports these 3 forecasting performance measures at the end of each forecast cycle in the following order of importance.  

1. *The forecast bias*, i.e. the fundamental forecast accuracy measure to underpin the total forecast’s credibility.
2. *The demand planning behaviours score* that measures how well people follow and support the forecasting process.  This is discussed in my blog on [measuring demand planning behaviours](https://5050forecasting.netlify.com/2019/09/measuring-demand-planning-behaviours/){target="_blank"}.
3. A **business specific forecast accuracy measure**, designed to measure the impact of the forecasting method on business performance.  This meaningful forecast accuracy measure ultimately should replace the MAPE score.  Suggested considerations such a measure are outlined in the next section, but it essentially reports the value that forecast accuracy is returning to the business.

**A business specific forecast accuracy should measure the impact on 4 areas of business impacts**  
There are 4 major impacts on an FMCG business with forecast accuracy:  

1. Better planning and allocation of sales and marketing investments.
2. Improved customer service and customer satisfaction through having the right stock in the right place at the right time.
3. Reduction in safety stock / working capital.
4. Reduction in logistical costs from warehousing and expediting.

The ideal forecast accuracy measure is customised to factor the benefit or cost implication of forecast accuracy on each of these 4 aspects.  Ideally, the forecast accuracy measure also factors a range of horizons, weighted according to the importance of the forecast horizons to the size of the opportunity in these 4 aspects.

Building such a forecast accuracy measure would be a significant undertaking, possibly involving cost accounting and almost certainly requiring broad assumptions and simplifications.  If such this progressive strategy was pursued, I recommend an iterative approach broadly outlined in [The Agile opportunity for Demand Planning Improvements](https://5050forecasting.netlify.com/2019/04/the-agile-opportunity-for-demand-planning-improvements/){target="_blank"}.

**When to apply the forecast accuracy measure**  
Demand planning should report on the forecast accuracy measure at the end of each cycle in the same way as MAPE is traditionally reported.  As stated, this is demand planning’s opportunity to report on the value of the forecast accuracy supplied to the business.

The other main application for the forecast accuracy measure is in process design and method selection.  For instance, if the demand planning department is selecting or refining the forecasting method, I recommend a process the compares the customised forecast accuracy measure achieves over multiple forecast cycles.  With this approach, we can predict the expected outcome of adopting a new method with the added benefit that it reflects a range of forecast horizons.

An example application from my career comes from when a major milk processor was evaluating alternative methods to forecast the milk volumes produced by dairy farmers.  In this evaluation, data scientists applied a MAPE measure to determine which method added most value to the business.  At least in this instance, the MAPE was calculated over a range of forecast cycles, reducing the risk of dumb luck adversely affecting the outcome one way or the other.  However, given the huge capital investment for milk processing, a customised forecast accuracy measure could have enabled a better decision had it factored a range of lags along with the subsequent and substantial business costs of forecast errors over those lags.

**Business Benefits of a customised forecast accuracy measure**  

1. More meaningful score of the forecast accuracy.
2. A forecast accuracy score that relates specifically to the business bottom line.
3. Better insights to enable better business decisions around sales and marketing investments and even capital expenditure.

To manage expectations, a the customised forecast accuracy measure won’t *immediately* deliver better forecast accuracy compared to the simple MAPE measure.  However, it will indirectly lead to more accurate forecasts by:

* Adding drive to improve forecasting behaviours by showing the true value of forecast accuracy.
* Enabling better and more meaningful refinement of forecasting methods and processes but equipping demand planners to estimate the true benefit of different approaches.

