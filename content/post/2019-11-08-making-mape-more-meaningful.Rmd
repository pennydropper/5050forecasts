---
title: Making MAPE More Meaningful
author: James Northrop
date: '2019-11-08'
slug: making-mape-more-meaningful
categories:
  - Process Maturity
  - demand planning and forecasting
tags:
  - Agile
  - Continuous Improvement
  - Finance
  - forecast
  - Sales and operation planning
  - Stakeholder management
  - Supply
  - uncertainty

draft: false
  
thumbnailImagePosition: left
thumbnailImage: /img/businessman-blindfold-hitting-target-bow-arrow-illustration-47052508.jpg # //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city-750.jpg
coverImage: /img/businessman-blindfold-hitting-target-bow-arrow-illustration-47052508.jpg # //d1u9biwaxjngwg.cloudfront.net/cover-image-showcase/city.jpg
coverSize: partial
metaAlignment: center
coverMeta: out

comments: true
favicon: "/img/favicon-32x32.png"
---
“You can’t manage what you can’t measure”, which is why it’s important for businesses to measure forecast accuracy.  For those unaware, MAPE (Mean Absolute Percentage Error) is one of the three most common forecast accuracy measures along with bias and MSE.  It is also used as a proxy for measuring the demand planning value-add in an FMCG businesses. That is, when the MAPE score is favourable (low), then it is assumed that demand planning’s value-add for a business must be high.

```{r setup, echo=FALSE, eval=FALSE, message=FALSE}
# require(tidyverse)
# require(ggthemes)
# require(lubridate)

knitr::opts_chunk$set(package.startup.message = FALSE)
library <- function(...) suppressPackageStartupMessages(base::library(...))

# suppressPackageStartupMessages(source("../../code/functions.R"))

library(tidyverse)
library(ggthemes)
library(lubridate)
library(magrittr)

```


```{r Ideal MAPE vs value-add, echo=FALSE, warning=FALSE, message=FALSE}

library(tidyverse)
library(ggthemes)
library(lubridate)
library(magrittr)

plot_mape_val_add <- function(p_title = "",  a = -2 * 1e3, b = 2 * 1e5, rand_f = 0.1) {
  # Builds a plot of relationship between MAPE scores and business value-add
  set.seed(1)
  
  mape_val_add <-
    # tibble(mape = runif(24, 25, 36)) %>% 
    tibble(mape = rweibull(24, 4, 1) %>%
             scales::rescale() %>% 
             map_dbl(~ 24 + . * 12)) %>%   # i.e. force MAPE to spread between 24 and 36
    mutate(val_add = b + a * mape - rand_f * a * rnorm(length(mape)))
  
  mape_val_rsq <- cor(mape_val_add$mape, mape_val_add$val_add) ^ 2
  
  mape_val_add %>% 
    ggplot(aes(x = mape / 100, y = val_add)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE) +
    theme_economist() +
    scale_x_continuous(labels = scales::percent) +
    scale_y_continuous(labels = NULL) +
    labs(title = p_title,
         subtitle = str_c("R-squared: ", format(mape_val_rsq, digits = 2)),
         x = "MAPE score", y = "Demand Planning Value-Add")
}

plot_mape_val_add("Presumed MAPE score and Business Value Relationship",
                  -2000, 20000, 1)


```


MAPE measures the forecast accuracy at a product or SKU level; it highlights forecasting issues where forecasts for individual products vary considerably from actual quantities, even if the bias for the total forecast is close to the ideal zero score.  For example, a MAPE score of 30% indicates that the forecast sales for individual products was either 30% above or below the actual sales (or visa versa if you divide actual by forecast).

Apart from bias, the other frequently used family of forecast scores is MSE, Mean Squared Error, which is more statistically pure though less meaningful to a wider audience than MAPE.  Compared to MAPE, MSE significantly penalises larger forecast errors.

I recommend my blog on [Forecasts are Deliberately Wrong](https://5050forecasting.netlify.com/2019/02/forecasts-are-deliberately-wrong/){target="_blank"} for the vital importance of measuring forecast accuracy bias.

## The Challenge  
MAPE (and MSE) are simple measures for forecast accuracy and definitely play a useful role. In fact, the simplicity is its key strength and why it is used so widely.  However, MAPE is a blunt instrument in a demand planning environment and holds 5 major weaknesses in how it is often applied.  In fact, these weaknesses mean that MAPE is a surprisingly poor measure of demand planning’s value-add.  In this piece, I’ll go onto describe each of these weaknesses and progress to suggesting an alternative.

```{r Alternative MAPE vs value-add, echo=FALSE, warning=FALSE}

plot_mape_val_add("More likely MAPE score and Business Value Relationship",
                  -2000, 20000, 10)

```


Incidentally, I’m classing forecast accuracy measures like WMAPE, SMAPE and r-squared as close cousins of either MAPE or MSE and subject to the same 4 weaknesses listed below.

**Strength 1: MAPE is simple and easy to calculate**  
By far the biggest strength of the MAPE statistic is that it is easy to calculate so long as you have stored the forecast at a SKU level and have accurate actual sales figures. Excel is capable of calculating the MAPE statistic in less than a minute, even when a pivot table is required to sum components.
 
**Weakness 1: Most MAPE measures only focus on 1 lag period**  
Most MAPE measures I’ve seen were based either on a 1-month or a 3-month lag, i.e. comparing the forecast 1 month or 3 months in advance of the forecast period.  The problem is that it disregards the importance of the forecasts with other lag periods.  If the other lag periods are disregarded, how can an organisation know how well the forecasting role is functioning?

```{r fcst sparklines, echo=FALSE, warning=FALSE}
month_start <- "1/1/2018"
ampl <- 100
rand_c <- 33
level <- 1000

set.seed(1234)

fcst_snaps <-
  crossing(tibble(fcst_mnth = 0:12), # only 12 months required
           tibble(mnth = 0:36)) %>%
  filter(mnth >= fcst_mnth, # Lag >= 0
         mnth - fcst_mnth <= 24) %>%   # 24 month horizon
  mutate(fcst = level + ampl * sin(mnth / 12 * 2 * pi) + rand_c * rnorm(length(mnth)),
         fcst_lag = mnth - fcst_mnth,
         fcst_mnth = dmy(month_start) + months(fcst_mnth),
         mnth = dmy(month_start) + months(mnth))

fcst_sparklines <- function(p_data) {
  # Draw sparklines for the forecast data
  
  p_data <-
    p_data %>%  
    mutate(fcst = round(fcst,0),
           fcst_min = min(fcst),
           fcst_max = max(fcst),
           facet_l = format(fcst_mnth, "%b-%y"))
  
  mnth_lbl <-
    p_data %>% 
    select(fcst_mnth, facet_l) %>% 
    distinct() %>% 
    deframe()
  
  mins <- group_by(p_data, fcst_mnth) %>% slice(which.min(fcst))
  maxs <- group_by(p_data, fcst_mnth) %>% slice(which.max(fcst))
  ends <- group_by(p_data, fcst_mnth) %>% filter(mnth == max(mnth))
  jan <- p_data %>% filter(mnth == ymd("2019/1/1"))
  # quarts <- p_data %>% group_by(fcst_mnth) %>%
  #   summarize(quart1 = quantile(fcst, 0.25),
  #             quart2 = quantile(fcst, 0.75)) %>%
  #   right_join(p_data)
  fcst_min <- min(p_data$fcst) %>% as.integer()
  
  p_data %>% 
    ggplot(., aes(x=mnth, y=fcst)) + 
    facet_grid(fcst_mnth ~ ., switch = "y", labeller = labeller(fcst_mnth = mnth_lbl)) + 
    geom_rect(aes(xmin = ymd("2019/1/1") - 15, xmax = ymd("2019/1/1") + 15, ymin = fcst_min, ymax = fcst_max + 20L), fill = 'grey90') +
    geom_line(size=0.3) +
    geom_point(data = mins, col = 'red', size = .5) +
    geom_point(data = maxs, col = 'blue', size = .5) +
    geom_point(data = jan, size = .8) +
    expand_limits(x = max(p_data$mnth) + (0.25 * (max(p_data$mnth) - min(p_data$mnth)))) +
    scale_x_date(breaks = dmy(month_start) + months(seq(0, 36, 12)), date_labels = "%b-%y") +
    scale_y_continuous(expand = c(0.1, 0)) +
    theme_tufte(base_size = 15, base_family = "Helvetica") +
    theme(axis.title=element_text(size = 8), axis.text.y = element_blank(), axis.text.x = element_text(size = 8),
          axis.ticks = element_blank(), strip.text.y = element_text(angle = 180, size = 8),
          # axis.title.y = element_text(size = 8),
          # axis.title = element_text(size = 8),
          plot.title = element_text(size = 10)) +
    
    geom_text(data = jan, aes(label = fcst), nudge_x = 50, size = 3) +
    labs(title = "Evolving Jan-19 forecast with each Demand Planning cycle",
         x = "Forecasted month", y = "Forecast cycle")
  
}

fcst_snaps %>% 
  fcst_sparklines()


```

```{r jan19 fcst bar, echo=FALSE, warning=FALSE}
# require(plotly)

fcst_snap_ggp <-
  fcst_snaps %>% 
  filter(mnth == dmy("1/1/2019")) %>% 
  mutate(mape_mnth = if_else(fcst_lag == 1, "MAPE", "not MAPE")) %>% 
  ggplot(aes(x = fcst_mnth, y = fcst)) +
  geom_bar(aes(fill = mape_mnth), stat = "identity") +
  labs(title = "Which forecast cycles contribute to Jan-19's MAPE result?",
       x = "Forecast month", y = "forecast quantity") +
  scale_fill_discrete("") +
  geom_text(data = fcst_snaps %>% filter(fcst_lag == 1, mnth == dmy("1/1/2019")), aes(label = fcst %>% scales::comma()), 
            nudge_y = 10) +
  coord_cartesian(ylim = c(800, 1100)) +
  theme_economist()

# plotly::ggplotly(fcst_snap_ggp)
fcst_snap_ggp

```
Even if a month’s forecast is accurate, the MAPE score disregards potentially critical weekly splits.  For instance, there is a significant difference if the entire month’s requirement is taken in the first week of the month as opposed to the last week of the month.  One example from my career occurred when a supermarket chain planned a promotion for a product imported from New Zealand, i.e. with extended lead times.  In this example, the promotion (just) met the required 1-month lead time but it resulted in an out-of-stock as it was required entirely in the first week of the month.

**Weakness 2: The delay between the forecasting activity and measuring the outcome of the activity**  
Since MAPE requires a lag, often 1 or 3 months, there is a delay between the activity that leads to a forecast and measuring the accuracy of that forecast, i.e. 1 or 3 months depending on the lag.  If MAPE is the primary measure for measuring the performance of the demand planning team, this delay or feedback lag hinders process improvement.  This delay is particularly difficult if MAPE is included in staff performance assessments and key performance outcomes.

![](/img/Delay to measuring MAPE.png)

**Weakness 3: A MAPE score is relatively meaningless to the wider audience**  
Credit goes to a former Sales Director (DC) for raising this issue.  When the monthly Demand Review presented the MAPE score achieved for the previous month, he asserted that it didn’t mean anything to him.  Quite reasonably, he asked what impact the MAPE score would have on the business bottom line.  The “bottom line” question is almost impossible to answer based purely on the MAPE score.

**Weakness 4: An individual MAPE score is an unreliable indicator of forecasting methods**  
Consider a situation where I forecast 50 cars to go past a marker in 30 minutes on a country road and 50 cars actually go past.  Does that mean that I have forecasted well?  Let’s say that 65 cars go past in the next 30 minutes compared to my forecast of 50, i.e. an error of 30%; has my forecasting method failed or am I just unlucky?

For instance, a robust method for forecasting the number of cars on a country road may consider the following:  

* Historical data on the number of cars that pass.
* Time of day.
* Exceptional circumstances, such as school holidays or whether there is a local football match down the road.
* The weather forecast.

FMCG environments will see fluctuating MAPE result; for example, they may fluctuate between 25% and 35%.  Does a one-off MAPE result of, say, 35%, indicate that forecasting performed particularly poorly in that month (the wrong forecast for the wrong reasons) or was it the result of dumb, bad luck (the wrong forecast for the right reasons)?  My point is that a forecast accuracy measure that focuses only on the quantities and not methods or behaviours then fails to factor the random aspect of an outcome.

```{r distn of MAPE scores, echo=FALSE, warning=FALSE}
set.seed(1234)

tibble(mape_int = seq(0.24, 0.36, 0.01)) %>% 
       mutate(mape_rescale = scales::rescale(mape_int),
              prob_int = dweibull(mape_rescale * 1.5, 4, 1),
              prob_int = prob_int / sum(prob_int)) %>% 
  # print
  ggplot(aes(x = mape_int , y = prob_int)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(labels = scales::percent, breaks = seq(0.24, 0.36, 0.02)) +
  scale_y_continuous(labels = scales::percent, limits = c(0, 0.2)) +
  labs(title = "Random effect of MAPE score", subtitle = "Range of possible results even with same forecasting method",
       x = "MAPE score", y = "Probability of MAPE outcome") +
  theme_economist()

```


**Weakness 4a: It takes a number of cycles of MAPE to confidently judge forecasting methods**  
Even with robust forecasting, some forecast error is likely.  It would take a number of forecast cycles to assess if the method was effective or whether the method included hidden flaws.

In a commercial terms, one month’s MAPE score is not a good indicator of the integrity of the forecasting method - it may just be that the system was particularly lucky or unlucky.  Even 2 consecutive months of a particularly good or bad MAPE score shouldn’t lead to a confident inference that the forecasting method is performing well.

**Weakness 5: MAPE scores depend heavily on their domain**  
One of my least favourite questions is "What was your MAPE score?". It is implicitly asking how well is demand planning run, but it overlooks the huge role that the demand planning domain has on the MAPE score.  Not only does MAPE depend on the industry, it depends on the geographic domain, the sales channel and even the retail model.  

One extreme contrast from my career is the MAPE score from kegged beer sales to the MAPE score from selling wine to wine distributors in East Asia.  Kegged beer sales in Australia around 2008 were incredibly consistent as there were few promotions and it was difficult for outlets to chop and change between products.  A MAPE score of 5% was normal.  In contrast, forecasting wine shipments to wholesalers in East Asia was remarkably challenging with many large but sporadic shipments.  MAPE scores were around 60% but in extreme months could even top 100%.  This was not a reflection of demand planning rigour - it was driven by the different domains.

Even when forecasting shipments to supermarket retailers, there were periods when a retailer were operating under "everyday low price" (EDLP) and shipments were more predictable.  At other times, the same retailers switched to "high-low pricing" (HiLo) when shipments became lumpier and pricing promotions were more opportunistic and harder to predict.
```{r Aust beer vs Asian wine, echo=FALSE, warning=FALSE, fig.height=4, fig.width=7}
sales_comp <-
  tibble(mnth = 0:36) %>% 
  mutate(act_keg = 1000 + 30 * sin(mnth / 12 * 2 * pi) + 5 * rnorm(length(mnth)),
         act_asia = 50 + 20 * sin(mnth / 12 * 2 * pi) + 15 * rnorm(length(mnth)) + rnorm(length(mnth), 400, 200) * rbinom(length(mnth), 1, 0.3),
         act_asia = pmax(act_asia, 0),
         mnth = dmy("1/1/2008") + months(mnth)) %>% 
  pivot_longer(cols = c(act_keg, act_asia), names_to = "market", values_to = "act_sales") %>% 
  mutate(market = fct_rev(market))

market_cv <-
  sales_comp %>% 
  group_by(market) %>% 
  summarise(sales_avg = mean(act_sales),
            sales_stddev = var(act_sales) ^ 0.5) %>% 
  mutate(coeff_var = sales_stddev / sales_avg) %>% 
  select(market, coeff_var) %>% 
  deframe()

sales_comp %>% 
  ggplot(aes(x = mnth)) +
  geom_line(aes(y = act_sales, colour = market)) +
  theme_economist() +
  scale_color_discrete("", labels = c("act_keg" = "Aust keg beer", "act_asia" = "Asian wine shipments")) +
  labs(title = "Vastly different sales profiles",
       subtitle = str_c("Indicative Australian keg sales to outlets vs Asian wine shipments to wholesalers",
                        "\nKeg Sales Coefficient of Variation: ", scales::percent(market_cv["act_keg"]),
                        "\nAsian Wine Shipments Coefficient of Variation: ", scales::percent(market_cv["act_asia"])),
       x = "", y = "Units shipped") +
  theme(legend.text = element_text(size = 10))


```


## A Solution
**I recommend reporting 3 forecasting performance measures**  
I recommend that demand planning reports these 3 forecasting performance measures at the end of each forecast cycle in the following order of importance.  

1. *The forecast bias*, i.e. the fundamental forecast accuracy measure to underpin the total forecast’s credibility.
2. *The demand planning behaviours score* that measures how well people follow and support the forecasting process.  This is discussed in my blog on [measuring demand planning behaviours](https://5050forecasting.netlify.com/2019/09/measuring-demand-planning-behaviours/){target="_blank"}.
3. A **_business specific forecast accuracy impact measure_**, (FAIM) designed to measure the impact of the forecasting method on business performance and ideally replaces the MAPE score.  Suggested inputs to FAIM are outlined in the next section, but it essentially reports the value that forecast accuracy is returning to the business.

**A business specific FAIM should factor the impact on 4 areas of business**  
There are 4 major areas of an FMCG business impacted of forecast accuracy:  

1. Better planning and allocation of sales and marketing investments.
2. Improved customer service and customer satisfaction through having the right stock in the right place at the right time.
3. Reduced safety stock / working capital.
4. Reduced logistical costs from warehousing and expediting.

The ideal FAIM is customised to factor the benefit or cost implication of forecast accuracy on each of these 4 areas  Ideally, the FAIM also factors a range of horizons, weighted according to the importance of the forecast horizons to the size of the opportunity in these 4 areas.

Building such a forecast accuracy measure would be a significant undertaking, possibly involving cost accounting and almost certainly requiring broad assumptions and simplifications.  If this progressive strategy was pursued, I recommend an iterative approach broadly outlined in [The Agile opportunity for Demand Planning Improvements](https://5050forecasting.netlify.com/2019/04/the-agile-opportunity-for-demand-planning-improvements/){target="_blank"}. The key to sensibly developing this value-adding measure is building it in bite-sized chunks with regular feedback and review. 

**When to apply FAIM**  
Demand planning should report on the FAIM at the end of each cycle in the same way as MAPE is traditionally reported.  As stated, this is demand planning’s opportunity to report on the value of the forecast accuracy supplied to the business.

The other main application for the FAIM is in process design and method selection.  For instance, if the demand planning department is selecting or refining the forecasting method, I recommend a process the compares the FAIM achieved over multiple forecast cycles.  With this approach, we can predict the expected outcome of adopting a new method with the added benefit that it reflects a range of forecast horizons.

An example application from my career comes from when a major milk processor was evaluating alternative methods to forecast the milk volumes produced by farms.  In this evaluation, the data scientists applied a MAPE measure to determine which method added most value to the business.  At least in this instance, the MAPE was calculated over a range of forecast cycles, reducing the risk of dumb luck adversely affecting the outcome one way or the other.  However, given the huge capital investment for milk processing, a customised FAIM could have enabled a better decision had it factored a range of lags along with the subsequent and substantial business costs of forecast errors over those lags.

## In short  
MAPE is easily the most frequently used measure of forecast accuracy in my direct experience, and it is often implicitly used to measure the performance of demand planning activities.  It's greatest strength is its simplicity, though it is a "blunt instrument" that comes with 5 major weaknesses.

I propose 2 measures to replace MAPE:  

1. The demand planning behaviours score, to be assessed immediately after the demand planning cycle.
2. A forecast accuracy impact measure (FAIM), which compares a range a forecast snapshots of actual sales and estimates the impact on forecast accuracy on a business stakeholders ranging from sales and marketing to supply.

To manage expectations, a the customised FAIM won’t *immediately* deliver better forecast accuracy compared to the simple MAPE measure.  However, it will indirectly lead to more accurate forecasts by:  

1. Providing a more meaningful score of the forecast accuracy, ideally relating it specifically to the business bottom line.
2. Better insights to enable better business decisions around sales and marketing investments and even capital expenditure.
3. Adding drive to improve forecasting behaviours by showing the true value of forecast accuracy.
4. Enabling better and more meaningful refinement of forecasting methods and processes allowing demand planners to estimate the true benefit of different approaches.

